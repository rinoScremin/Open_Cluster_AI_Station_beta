---

# GQA Cluster Transformer

Distributed **LLaMA-style Transformer** with **Grouped Query Attention (GQA)** and **KV-cache**, powered by a custom ZeroMQ-based compute cluster backend.

This project enables running transformer attention and MLP layers across multiple nodes (CPU/GPU) using a matrix-splitting cluster system.

---

## ðŸš€ Features

* âœ… LLaMA-style architecture
* âœ… Grouped Query Attention (GQA)
* âœ… Rotary Positional Embeddings (RoPE)
* âœ… Sliding-window KV cache
* âœ… RMSNorm (LLaMA style)
* âœ… Top-p (nucleus) sampling
* âœ… Repetition penalty
* âœ… Streaming token callback support
* âœ… Local HuggingFace or SentencePiece tokenizer support
* âœ… Distributed matrix compute via `cluster_matrix_v1`
* âœ… Deterministic cuDNN + fixed seeds

---

## ðŸ§  Architecture Overview

The core model class:

```
llama_cluster_transformer
```

It:

* Uses a model handler (`hugging_face_model_handler`)
* Distributes Q/K/V/O projections across cluster nodes
* Maintains KV cache per layer
* Applies RoPE during attention
* Runs attention + MLP blocks
* Produces logits using tied embeddings or LM head

---

## ðŸ“¦ Requirements

### Python

* Python 3.9+
* PyTorch
* Transformers
* SentencePiece
* NumPy

### Internal Dependencies

This project requires:

* `cluster_matrix_v1`
* `cluster_zmq`
* `gguf_parser`
* `transformer_model_handler`

These must exist in your project structure.

---

## ðŸ“ Project Structure (Expected)

```
project_root/
â”‚
â”œâ”€â”€ GQA_cluster_transformer.py
â”œâ”€â”€ transformer_model_handler.py
â”œâ”€â”€ gguf_parser.py
â”œâ”€â”€ cluster_matrix/
â”‚   â””â”€â”€ cluster_matrix_v1.py
â”‚
â””â”€â”€ llm_models/
```

---

## ðŸ”§ Environment Variables

You can control KV cache behavior:

```
OPEN_CLUSTER_KV_WINDOW=2048
OPEN_CLUSTER_KV_CACHE_DTYPE=fp16  # options: fp16, bf16, fp32
```

---

## ðŸ§© Tokenizer Support

The `Tokenizer` class supports:

### 1ï¸âƒ£ SentencePiece (`.model`)

Uses:

* `SentencePieceProcessor`

### 2ï¸âƒ£ HuggingFace (`tokenizer.json` or model directory)

Uses:

* `transformers.AutoTokenizer`

Special tokens are normalized automatically.

---

## ðŸ— Model Initialization

Example:

```python
from GQA_cluster_transformer import llama_cluster_transformer, Tokenizer
from transformer_model_handler import hugging_face_model_handler

tokenizer = Tokenizer("path_to_tokenizer_or_model")

model_handler = hugging_face_model_handler(
    model_path="path_to_model",
    ...
)

model = llama_cluster_transformer(tokenizer, model_handler)
```

---

## ðŸ§ª Text Generation

```python
output = model.generate(
    prompts="Explain GQA in simple terms.",
    max_gen_len=200,
    temperature=0.8,
    top_p=0.95
)

print(output[0])
```

---

## âš¡ Cluster Execution

The distributed path runs through:

```
run_QKV_mlp_cluster()
```

This method:

1. RMSNorm
2. Distributes Q/K/V projections across cluster
3. Applies RoPE
4. Updates KV cache
5. Runs attention
6. Applies post-attention RMSNorm
7. Runs SwiGLU MLP
8. Residual connections

Matrix splitting is controlled by:

* `CPU_GPU_select_list`
* `percentages`
* `backend_select_list`
* `split_dim`

All are supplied by the model handler.

---

## ðŸ§  KV Cache

* Sliding window implementation
* Configurable window size
* Supports fp16 / bf16 / fp32
* Wrap-around circular buffer logic

Reset automatically per `generate()` call.

Manual clear:

```python
model.clear_kv_cache()
```

---

## ðŸ§® Attention Details

* Supports `torch.nn.functional.scaled_dot_product_attention` if available
* Fallback manual masked softmax
* Uses LLaMA-style:

  * RoPE
  * RMSNorm (pre + post attention)
  * SwiGLU MLP

---

## ðŸ›‘ Stopping Conditions

Supports:

* `stop_ids`
* `stop_words`
* Token ID stop sequences
* Default EOS handling
* Special token filtering

---

## ðŸ“¡ Streaming Tokens

You can stream generated tokens:

```python
def on_token(batch_idx, token_id, token_text):
    print(token_text, end="", flush=True)

model.generate(
    prompts="Tell me a story",
    max_gen_len=200,
    on_token=on_token
)
```

---

## ðŸ”’ Determinism

The script enforces:

```python
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```

For reproducible runs.

---

## âš  Current Limitations

* Batch size >1 not supported in `run_QKV_mlp`
* Requires proper cluster backend setup
* Assumes LLaMA-style weight layout
* Memory usage scales with KV window Ã— layers

---

## ðŸŽ¯ Intended Use

This is designed for:

* Experimental distributed transformer inference
* Custom LLM cluster systems
* Performance experimentation
* Matrix-splitting research
* GQA architecture testing

---

## ðŸ›  Debugging

Debug log path:

```
output_logs/transformer_debug.log
```

Cluster test matrices (optional):

```
cluster_matrix/test_model_matrices/
```
